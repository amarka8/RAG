{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code is responsible for building the index for our text corpus. We use BERT for our embeddings model and tokenizer, and we use FAISS cosine similarity to index our normalized vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarkanaka/miniconda3/envs/faiss_1.8.0_new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 11656\n",
      "Loading existing vectors: data/musique/musique_facebook_contriever_proposition_vectors_norm.npy\n",
      "Vectors loaded: 11656\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# import sys\n",
    "\n",
    "# from src.processing import mean_pooling, mean_pooling_embedding_with_normalization\n",
    "\n",
    "# sys.path.append('.')\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "# from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "#either musique dataset or 2wikimultihopqa dataset\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--dataset',type = str)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "dim = 768\n",
    "#normalize embeddings before building index using inner product. Note that maximal inner product with normalized embeddings is equivalent to cosine similarity \n",
    "norm = True\n",
    "\n",
    "\n",
    "\"\"\"CHANGE THIS LINE TO USE OTHER DATASET\"\"\"\n",
    "dataset = 'musique'\n",
    "\"\"\"CHANGE THIS LINE TO USE OTHER DATASET\"\"\"\n",
    "\n",
    "\n",
    "model_label = 'facebook_contriever'\n",
    "vector_path = f'data/{dataset}/{dataset}_{model_label}_proposition_vectors_norm.npy'\n",
    "index_path = f'data/{dataset}/{dataset}_{model_label}_proposition_ip_norm.index'\n",
    "if(os.path.isfile(vector_path)):\n",
    "    vectors = np.load(vector_path)\n",
    "if dataset == 'musique':\n",
    "    corpus = json.load(open('data/musique_proposition_corpus.json', 'r'))\n",
    "elif dataset == '2wikimultihopqa':\n",
    "    corpus = json.load(open('data/2wikimultihopqa_proposition_corpus.json', 'r'))\n",
    "corpus_contents = []\n",
    "for item in corpus:\n",
    "    corpus_contents.append(item['title'] + '\\n' + item['propositions'])\n",
    "print('corpus size: {}'.format(len(corpus_contents)))\n",
    "\n",
    "#create sentence-level embeddings using mean-pooling and normalize to prepare for cosine similarity indexing\n",
    "#note: UPDATE TO USE distributedDataParallel\n",
    "\n",
    "def mean_pooling(tokenEmbeddings, paddingInfo):\n",
    "    tokenEmbeddingsNoPad = tokenEmbeddings.masked_fill(~paddingInfo[...,None].bool(), 0)\n",
    "    sentenceEmbeddings = tokenEmbeddingsNoPad.sum(dim = 1) / paddingInfo.sum(dim = 1)[...,None]\n",
    "    return sentenceEmbeddings\n",
    "\n",
    "def mean_pooling_embedding_with_normalization(batch_str, tokenizer, model):\n",
    "    mps_device = torch.device(\"mps\") \n",
    "    encoding = tokenizer(batch_str, padding=True, truncation=True, return_tensors='pt').to(mps_device)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    input_ids = input_ids.to(mps_device)\n",
    "    attention_mask = attention_mask.to(mps_device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    sentenceEmbeddings = mean_pooling(outputs[0], attention_mask)\n",
    "    sentenceEmbeddingsNorm = sentenceEmbeddings.divide(torch.linalg.norm(sentenceEmbeddings,dim = 1)[...,None])\n",
    "    return sentenceEmbeddingsNorm\n",
    "\n",
    "if os.path.isfile(vector_path):\n",
    "    print('Loading existing vectors:', vector_path)\n",
    "    vectors = np.load(vector_path)\n",
    "    print('Vectors loaded:', len(vectors))\n",
    "\n",
    "else:\n",
    "    # load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
    "    model = AutoModel.from_pretrained('facebook/contriever')\n",
    "    # Check if multiple GPUs are available and if so, use them all\n",
    "    if (torch.cuda.is_available()):\n",
    "        device = torch.device(\"cuda\")    \n",
    "        model.to(device)\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    #test batch size = 16 and batch size = 32 \n",
    "    batch_size = 16\n",
    "    vectors = np.zeros((len(corpus_contents), dim))\n",
    "    #get batch_size number of entries from corpus_contents, tokenize and embed them in 768 dimensional space\n",
    "    for idx in range(0, len(corpus_contents), batch_size):\n",
    "        end_idx = min(idx + batch_size, len(corpus_contents))\n",
    "        seqs = corpus_contents[idx:end_idx]\n",
    "        try:\n",
    "            #read above comments to understand what this function does\n",
    "            batch_embeddings = mean_pooling_embedding_with_normalization(seqs, tokenizer, model)\n",
    "        except Exception as e:\n",
    "            batch_embeddings = torch.zeros((len(seqs), dim))\n",
    "            print(f'Error at {idx}:', e)\n",
    "        vectors[idx:end_idx] = batch_embeddings.detach().to('cpu').numpy()\n",
    "    print(\"Type of vectors is {}\".format(type(vectors)))\n",
    "    fp = open(vector_path, 'wb')\n",
    "    np.save(fp, vectors)\n",
    "    fp.close()\n",
    "    print('vectors saved to {}'.format(vector_path))\n",
    "\n",
    "    #using FAISS on CPU (GPU support unavailable for mac)\n",
    "    if os.path.isfile(index_path):\n",
    "            print('index file already exists:', index_path)\n",
    "            print('index size: {}'.format(faiss.read_index(index_path).ntotal))\n",
    "    else:\n",
    "        print('Building index...')\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        vectors = vectors.astype('float32')\n",
    "        index.add(vectors)\n",
    "\n",
    "        # save faiss index to file\n",
    "        # fp = open(index_path, 'w')\n",
    "        faiss.write_index(index, index_path)\n",
    "        print('index saved to {}'.format(index_path))\n",
    "        print('index size: {}'.format(index.ntotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check that Indexing Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m D, I \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39msearch(vectors[:\u001b[38;5;241m5\u001b[39m], \u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# sanity check\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(I)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(D)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "D, I = index.search(vectors[:5], 4) # sanity check\n",
    "print(I)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block if you want to know statistics about approximate number of tokens in each line of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525.25\n",
      "31.25\n",
      "149.56335792724778\n"
     ]
    }
   ],
   "source": [
    "# total_len = 0\n",
    "# max_len = 0\n",
    "# min_len = 1000000\n",
    "# for line in corpus_contents:\n",
    "#     total_len += len(line)\n",
    "#     if len(line) > max_len:\n",
    "#         max_len = len(line)\n",
    "#     if len(line) < min_len:\n",
    "#         min_len = len(line)\n",
    "# print(max_len / 4)\n",
    "# print(min_len / 4)\n",
    "# print((total_len / len(corpus_contents)) / 4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks of code are responsible for evaluating our RAG system's retrieval on our two corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "class DocumentRetriever:\n",
    "    @abstractmethod\n",
    "    def rank_docs(self, query: str, top_k: int):\n",
    "        \"\"\"\n",
    "        Rank the documents in the corpus based on the given query\n",
    "        :param query:\n",
    "        :param top_k: \n",
    "        :return: ranks and scores of the retrieved documents\n",
    "        \"\"\"\n",
    "class DPRRetriever(DocumentRetriever):\n",
    "    def __init__(self, model_name: str, faiss_index: str, corpus):\n",
    "        \"\"\"\n",
    "\n",
    "        :param model_name:\n",
    "        :param faiss_index: The path to the faiss index\n",
    "        \"\"\"\n",
    "        device  = torch.device(\"mps\") \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.faiss_index = faiss_index\n",
    "        self.corpus = corpus\n",
    "        self.device = device\n",
    "\n",
    "    def rank_docs(self, query: str, top_k: int):\n",
    "        # query_embedding = mean_pooling_embedding(query, self.tokenizer, self.model, self.device)\n",
    "        with torch.no_grad():\n",
    "            query_embedding = mean_pooling_embedding_with_normalization(query, self.tokenizer, self.model).detach().cpu().numpy()\n",
    "        inner_product, corpus_idx = self.faiss_index.search(query_embedding, top_k)\n",
    "        return corpus_idx.tolist()[0], inner_product.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "def num_tokens_by_tiktoken(text: str):\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def merge_elements_with_same_first_line(elements, prefix='Wikipedia Title: '):\n",
    "    merged_dict = {}\n",
    "\n",
    "    # Iterate through each element in the list\n",
    "    for element in elements:\n",
    "        # Split the element into lines and get the first line\n",
    "        lines = element.split('\\n')\n",
    "        first_line = lines[0]\n",
    "\n",
    "        # Check if the first line is already a key in the dictionary\n",
    "        if first_line in merged_dict:\n",
    "            # Append the current element to the existing value\n",
    "            merged_dict[first_line] += \"\\n\" + element.split(first_line, 1)[1].strip('\\n')\n",
    "        else:\n",
    "            # Add the current element as a new entry in the dictionary\n",
    "            merged_dict[first_line] = prefix + element\n",
    "\n",
    "    # Extract the merged elements from the dictionary\n",
    "    merged_elements = list(merged_dict.values())\n",
    "    return merged_elements\n",
    "\n",
    "ircot_reason_instruction = 'You serve as an intelligent assistant, adept at facilitating users through complex, multi-hop reasoning across multiple documents. This task is illustrated through demonstrations, each consisting of a document set paired with a relevant question and its multi-hop reasoning thoughts. Your task is to generate one thought for current step, DON\\'T generate the whole thoughts at once! If you reach what you believe to be the final step, start with \"So the answer is:\".'\n",
    "\n",
    "def reason_step(dataset, few_shot: list, query: str, passages: list, thoughts: list, client):\n",
    "    \"\"\"\n",
    "    Given few-shot samples, query, previous retrieved passages, and previous thoughts, generate the next thought with LangChain LLM.\n",
    "    The generated thought is used for further retrieval step.\n",
    "    :return: next thought\n",
    "    \"\"\"\n",
    "    prompt_demo = ''\n",
    "\n",
    "    prompt_user = ''\n",
    "    if dataset in ['hotpotqa']:\n",
    "        passages = merge_elements_with_same_first_line(passages)\n",
    "    for passage in passages:\n",
    "        prompt_user += f'Wikipedia Title: {passage}\\n\\n'\n",
    "    prompt_user += f'Question: {query}\\nThought:' + ' '.join(thoughts)\n",
    "\n",
    "    for sample in few_shot:\n",
    "        cur_sample = f'{sample[\"document\"]}\\n\\nQuestion: {sample[\"question\"]}\\nThought: {sample[\"thought_and_answer\"]}\\n\\n'\n",
    "        if num_tokens_by_tiktoken(ircot_reason_instruction + prompt_demo + cur_sample + prompt_user) < 15000:\n",
    "            prompt_demo += cur_sample\n",
    "\n",
    "    messages = ChatPromptTemplate.from_messages([SystemMessage(ircot_reason_instruction + '\\n\\n' + prompt_demo),\n",
    "                                                 HumanMessage(prompt_user)]).format_prompt()\n",
    "\n",
    "    try:\n",
    "        chat_completion = client.invoke(messages.to_messages())\n",
    "        response_content = chat_completion.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return ''\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_step(query: str, corpus, top_k: int, retriever: DocumentRetriever, dataset: str):\n",
    "    doc_ids, scores = retriever.rank_docs(query, top_k=top_k)\n",
    "    if dataset in ['hotpotqa']:\n",
    "        retrieved_passages = []\n",
    "        for doc_id in doc_ids:\n",
    "            key = list(corpus.keys())[doc_id]\n",
    "            retrieved_passages.append(key + '\\n' + ''.join(corpus[key]))\n",
    "    elif dataset in ['musique', '2wikimultihopqa']:        \n",
    "        retrieved_passages = [corpus[doc_id]['title'] + '\\n' + corpus[doc_id]['text'] for doc_id in doc_ids]\n",
    "    else:\n",
    "        raise NotImplementedError(f'Dataset {dataset} not implemented')\n",
    "    return retrieved_passages, scores\n",
    "\n",
    "def process_sample(idx, sample, dataset, top_k, k_list,max_steps, few_shot_samples, corpus, retriever, client, processed_ids):\n",
    "    # Check if the sample has already been processed\n",
    "    if dataset in ['hotpotqa', '2wikimultihopqa']:\n",
    "        sample_id = sample['_id']\n",
    "    elif dataset in ['musique']:\n",
    "        sample_id = sample['id']\n",
    "    else:\n",
    "        raise NotImplementedError(f'Dataset {dataset} not implemented')\n",
    "    if sample_id in processed_ids:\n",
    "        return  # Skip already processed samples\n",
    "    else:\n",
    "        processed_ids.add(sample_id)\n",
    "\n",
    "    # Perform retrieval and reasoning steps\n",
    "    query = sample['question']\n",
    "    #uncomment this line if you want to see the questions being asked\n",
    "    # print(query)\n",
    "    retrieved_passages, scores = retrieve_step(query, corpus, top_k, retriever, dataset)\n",
    "\n",
    "    thoughts = []\n",
    "    retrieved_passages_dict = {passage: score for passage, score in zip(retrieved_passages, scores)}\n",
    "    it = 1\n",
    "    for it in range(1, max_steps):\n",
    "        print(\"in IRCOT loop\")\n",
    "        new_thought = reason_step(dataset, few_shot_samples, query, retrieved_passages[:top_k], thoughts, client)\n",
    "        thoughts.append(new_thought)\n",
    "        if 'So the answer is:' in new_thought:\n",
    "            break\n",
    "        new_retrieved_passages, new_scores = retrieve_step(new_thought, corpus, top_k, retriever, dataset)\n",
    "\n",
    "        for passage, score in zip(new_retrieved_passages, new_scores):\n",
    "            if passage in retrieved_passages_dict:\n",
    "                retrieved_passages_dict[passage] = max(retrieved_passages_dict[passage], score)\n",
    "            else:\n",
    "                retrieved_passages_dict[passage] = score\n",
    "\n",
    "        retrieved_passages, scores = zip(*retrieved_passages_dict.items())\n",
    "\n",
    "        sorted_passages_scores = sorted(zip(retrieved_passages, scores), key=lambda x: x[1], reverse=True)\n",
    "        retrieved_passages, scores = zip(*sorted_passages_scores)\n",
    "    # end iteration\n",
    "\n",
    "    # calculate recall\n",
    "    if dataset in ['hotpotqa']:\n",
    "        gold_passages = [item for item in sample['supporting_facts']]\n",
    "        gold_items = set([item[0] for item in gold_passages])\n",
    "        retrieved_items = [passage.split('\\n')[0].strip() for passage in retrieved_passages]\n",
    "    elif dataset in ['musique']:\n",
    "        gold_passages = [item for item in sample['paragraphs'] if item['is_supporting']]\n",
    "        # print(gold_passages)\n",
    "        gold_items = set([item['title'] + '\\n' + item['paragraph_text'] for item in gold_passages])\n",
    "        # print(gold_items)\n",
    "        retrieved_items = retrieved_passages\n",
    "        # print(retrieved_passages:10)\n",
    "    elif dataset in ['2wikimultihopqa']:\n",
    "        gold_passages = [item for item in sample['supporting_facts']]\n",
    "        gold_items = set([item[0] for item in gold_passages])\n",
    "        retrieved_items = [passage.split('\\n')[0].strip() for passage in retrieved_passages]\n",
    "    else:\n",
    "        raise NotImplementedError(f'Dataset {dataset} not implemented')\n",
    "\n",
    "    recall = dict()\n",
    "    print(f'idx: {idx + 1} ', end='')\n",
    "    for k in k_list:\n",
    "        # in the top k retrieved docs, sum the number of true positives (gold items) found and divide by the total number of true positives\n",
    "        # fraction of retrieved passages found\n",
    "        recall[k] = sum(1 for t in gold_items if t in retrieved_items[:k]) / len(gold_items)\n",
    "    return idx, recall, retrieved_passages, thoughts, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prompt(file_path: str, has_context=True):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Split the content by the metadata pattern\n",
    "    parts = content.split('# METADATA: ')\n",
    "    parsed_data = []\n",
    "    if has_context:\n",
    "        for part in parts[1:]:  # Skip the first split as it will be empty\n",
    "            metadata_section, rest_of_data = part.split('\\n', 1)\n",
    "            metadata = json.loads(metadata_section)\n",
    "            document_sections = rest_of_data.strip().split('\\n\\nQ: ')\n",
    "            document_text = document_sections[0].strip()\n",
    "            qa_pair = document_sections[1].split('\\nA: ')\n",
    "            question = qa_pair[0].strip()\n",
    "            thought_and_answer = qa_pair[1].strip().split('So the answer is: ')\n",
    "            thought = thought_and_answer[0].strip()\n",
    "            answer = thought_and_answer[1].strip()\n",
    "\n",
    "            parsed_data.append({\n",
    "                'metadata': metadata,\n",
    "                'document': document_text,\n",
    "                'question': question,\n",
    "                'thought_and_answer': qa_pair[1].strip(),\n",
    "                'thought': thought,\n",
    "                'answer': answer\n",
    "            })\n",
    "    else:\n",
    "        for part in parts[1:]:\n",
    "            metadata_section, rest_of_data = part.split('\\n', 1)\n",
    "            metadata = json.loads(metadata_section)\n",
    "            s = rest_of_data.split('\\n')\n",
    "            question = s[0][3:].strip()\n",
    "            thought_and_answer = s[1][3:].strip().split('So the answer is: ')\n",
    "            thought = thought_and_answer[0].strip()\n",
    "            answer = thought_and_answer[1].strip()\n",
    "\n",
    "            parsed_data.append({\n",
    "                'metadata': metadata,\n",
    "                'question': question,\n",
    "                'thought_and_answer': s[1][3:].strip(),\n",
    "                'thought': thought,\n",
    "                'answer': answer\n",
    "            })\n",
    "\n",
    "    return parsed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = 'gpt-3.5-turbo-1106'\n",
    "llm = 'openai'\n",
    "\"\"\"\n",
    "User-inputted args\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "For 2wikimultihopqa, change max_steps to 2, num_demo to 1 to perform multistep retrieval.\n",
    "For musique, change max_steps to 4, num_demo 1 to perform multistep retrieval.\n",
    "\"\"\"\n",
    "max_steps = 1\n",
    "num_demo = 0\n",
    "top_k = 8\n",
    "#load dataset\n",
    "if dataset == 'musique':\n",
    "    data = json.load(open('data/musique.json', 'r'))\n",
    "    corpus = json.load(open('data/musique_corpus.json', 'r'))\n",
    "    prompt_path = 'data/ircot_prompts/musique/gold_with_3_distractors_context_cot_qa_codex.txt'\n",
    "    max_steps = max_steps if max_steps is not None else 4\n",
    "elif dataset == '2wikimultihopqa':\n",
    "    data = json.load(open('data/2wikimultihopqa.json', 'r'))\n",
    "    corpus = json.load(open('data/2wikimultihopqa_corpus.json', 'r'))\n",
    "    prompt_path = 'data/ircot_prompts/2wikimultihopqa/gold_with_3_distractors_context_cot_qa_codex.txt'\n",
    "    max_steps = max_steps if max_steps is not None else 2\n",
    "else:\n",
    "    raise NotImplementedError(f'Dataset {dataset} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "#Create OpenAI Client\n",
    "client = ChatOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"), model=llm_model, temperature=0.0, max_retries=5, timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of demo: 0\n"
     ]
    }
   ],
   "source": [
    "#load num_demo distractor passages\n",
    "few_shot_samples = parse_prompt(prompt_path)\n",
    "few_shot_samples = few_shot_samples[:num_demo]\n",
    "print('num of demo:', len(few_shot_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_ensemble_str = 'doc_ensemble' if doc_ensemble else 'no_ensemble'\n",
    "doc_ensemble_str = ''\n",
    "alt_model_label = 'facebook_contriever'\n",
    "if max_steps > 1:\n",
    "    output_path = f'output/ircot/{dataset}_{alt_model_label}_demo_{num_demo}_{llm_model}_{doc_ensemble_str}_step_{max_steps}_top_{top_k}.json'\n",
    "else:  # only one step\n",
    "    top_k = 100\n",
    "    output_path = f'output/proposition_{dataset}_{alt_model_label}_{doc_ensemble_str}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'musique':\n",
    "    faiss_index = faiss.read_index('data/musique/musique_facebook_contriever_proposition_ip_norm.index')\n",
    "else:\n",
    "    faiss_index = faiss.read_index('data/2wikimultihopqa/2wikimultihopqa_facebook_contriever_proposition_ip_norm.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label = 'facebook/contriever'\n",
    "retriever = DPRRetriever(model_label, faiss_index, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [1, 2, 5, 10, 15, 20, 30, 50, 100]\n",
    "total_recall = {k: 0 for k in k_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Submit tasks to the executor\n",
    "    futures = [executor.submit(process_sample,idx, sample, dataset, top_k, k_list,max_steps, few_shot_samples, corpus, retriever, client, processed_ids) for idx, sample in enumerate(data)]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        idx, recall, retrieved_passages, thoughts, it = future.result()\n",
    "\n",
    "        # print metrics\n",
    "        for k in k_list:\n",
    "            total_recall[k] += recall[k]\n",
    "            print(f'R@{k}: {total_recall[k] / (idx + 1):.4f} ', end='')\n",
    "        print()\n",
    "        if max_steps > 1:\n",
    "            print('[ITERATION]', it, '[PASSAGE]', len(retrieved_passages), '[THOUGHT]', thoughts)\n",
    "\n",
    "        # record results\n",
    "        results[idx]['retrieved'] = retrieved_passages\n",
    "        results[idx]['recall'] = recall\n",
    "        results[idx]['thoughts'] = thoughts\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            f = open(output_path, 'w')\n",
    "            json.dump(results, f)\n",
    "            f.close()\n",
    "\n",
    "# save results\n",
    "f = open(output_path, 'w')\n",
    "json.dump(results, f)\n",
    "f.close()\n",
    "print(f'Saved results to {output_path}')\n",
    "for k in k_list:\n",
    "    #average recall (across 1,000 questions for musique)\n",
    "    print(f'R@{k}: {total_recall[k] / len(data):.4f} ', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_1.8.0_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
