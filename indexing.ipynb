{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 11656\n",
      "Type of vectors is <class 'numpy.ndarray'>\n",
      "vectors saved to data/musique/musique_facebook_contriever_proposition_vectors_norm.npy\n",
      "Building index...\n",
      "index saved to data/musique/musique_facebook_contriever_proposition_ip_norm.index\n",
      "index size: 11656\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This block of code is responsible for building the index for our text corpus. We use BERT for our embeddings model and tokenizer, \n",
    "and we use FAISS cosine similarity to index our normalized vectors \n",
    "\"\"\"\n",
    "import os\n",
    "# import sys\n",
    "\n",
    "# from src.processing import mean_pooling, mean_pooling_embedding_with_normalization\n",
    "\n",
    "# sys.path.append('.')\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "# from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "#either musique dataset or 2wikimultihopqa dataset\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--dataset',type = str)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "dim = 768\n",
    "#normalize embeddings before building index using inner product. Note that maximal inner product with normalized embeddings is equivalent to cosine similarity \n",
    "norm = True\n",
    "#CHANGE THIS\n",
    "dataset = 'musique'\n",
    "model_label = 'facebook_contriever'\n",
    "vector_path = f'data/{dataset}/{dataset}_{model_label}_proposition_vectors_norm.npy'\n",
    "index_path = f'data/{dataset}/{dataset}_{model_label}_proposition_ip_norm.index'\n",
    "if(os.path.isfile(vector_path)):\n",
    "    vectors = np.load(vector_path)\n",
    "if dataset == 'musique':\n",
    "    corpus = json.load(open('data/musique_proposition_corpus.json', 'r'))\n",
    "elif dataset == '2wikimultihopqa':\n",
    "    corpus = json.load(open('data/2wikimultihopqa_proposition_corpus.json', 'r'))\n",
    "corpus_contents = []\n",
    "for item in corpus:\n",
    "    corpus_contents.append(item['title'] + '\\n' + item['propositions'])\n",
    "print('corpus size: {}'.format(len(corpus_contents)))\n",
    "\n",
    "#create sentence-level embeddings using mean-pooling and normalize to prepare for cosine similarity indexing\n",
    "#note: UPDATE TO USE distributedDataParallel\n",
    "\n",
    "def mean_pooling(tokenEmbeddings, paddingInfo):\n",
    "    tokenEmbeddingsNoPad = tokenEmbeddings.masked_fill(~paddingInfo[...,None].bool(), 0)\n",
    "    sentenceEmbeddings = tokenEmbeddingsNoPad.sum(dim = 1) / paddingInfo.sum(dim = 1)[...,None]\n",
    "    return sentenceEmbeddings\n",
    "\n",
    "def mean_pooling_embedding_with_normalization(batch_str, tokenizer, model):\n",
    "    mps_device = torch.device(\"mps\") \n",
    "    encoding = tokenizer(batch_str, padding=True, truncation=True, return_tensors='pt').to(mps_device)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    input_ids = input_ids.to(mps_device)\n",
    "    attention_mask = attention_mask.to(mps_device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    sentenceEmbeddings = mean_pooling(outputs[0], attention_mask)\n",
    "    sentenceEmbeddingsNorm = sentenceEmbeddings.divide(torch.linalg.norm(sentenceEmbeddings,dim = 1)[...,None])\n",
    "    return sentenceEmbeddingsNorm\n",
    "\n",
    "if os.path.isfile(vector_path):\n",
    "    print('Loading existing vectors:', vector_path)\n",
    "    vectors = np.load(vector_path)\n",
    "    print('Vectors loaded:', len(vectors))\n",
    "\n",
    "else:\n",
    "    # load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
    "    model = AutoModel.from_pretrained('facebook/contriever')\n",
    "    # Check if multiple GPUs are available and if so, use them all\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    else:\n",
    "        # print(\"device available\")\n",
    "        mps_device = torch.device(\"mps\")    \n",
    "        model.to(mps_device)\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    #test batch size = 16 and batch size = 32 \n",
    "    batch_size = 16\n",
    "    vectors = np.zeros((len(corpus_contents), dim))\n",
    "    #get batch_size number of entries from corpus_contents, tokenize and embed them in 768 dimensional space\n",
    "    for idx in range(0, len(corpus_contents), batch_size):\n",
    "        end_idx = min(idx + batch_size, len(corpus_contents))\n",
    "        seqs = corpus_contents[idx:end_idx]\n",
    "        try:\n",
    "            #read above comments to understand what this function does\n",
    "            batch_embeddings = mean_pooling_embedding_with_normalization(seqs, tokenizer, model)\n",
    "        except Exception as e:\n",
    "            batch_embeddings = torch.zeros((len(seqs), dim))\n",
    "            print(f'Error at {idx}:', e)\n",
    "        vectors[idx:end_idx] = batch_embeddings.detach().to('cpu').numpy()\n",
    "    print(\"Type of vectors is {}\".format(type(vectors)))\n",
    "    fp = open(vector_path, 'wb')\n",
    "    np.save(fp, vectors)\n",
    "    fp.close()\n",
    "    print('vectors saved to {}'.format(vector_path))\n",
    "\n",
    "    #using FAISS on CPU (GPU support unavailable for mac)\n",
    "    if os.path.isfile(index_path):\n",
    "            print('index file already exists:', index_path)\n",
    "            print('index size: {}'.format(faiss.read_index(index_path).ntotal))\n",
    "    else:\n",
    "        print('Building index...')\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        vectors = vectors.astype('float32')\n",
    "        index.add(vectors)\n",
    "\n",
    "        # save faiss index to file\n",
    "        # fp = open(index_path, 'w')\n",
    "        faiss.write_index(index, index_path)\n",
    "        print('index saved to {}'.format(index_path))\n",
    "        print('index size: {}'.format(index.ntotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check that Indexing Worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  9 19  1]\n",
      " [ 1  5  2  7]\n",
      " [ 2  1  7 10]\n",
      " [ 3 10 17  7]\n",
      " [ 4 15 13 18]]\n",
      "[[1.         0.5485658  0.47989315 0.45996296]\n",
      " [1.0000001  0.5450722  0.5391499  0.5123867 ]\n",
      " [0.9999999  0.5391499  0.4831297  0.46856448]\n",
      " [1.         0.8503537  0.83556557 0.7047421 ]\n",
      " [1.0000001  0.5722819  0.4969578  0.49316454]]\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(vectors[:5], 4) # sanity check\n",
    "print(I)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block if you want to know statistics about approximate token size of each line in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525.25\n",
      "31.25\n",
      "149.56335792724778\n"
     ]
    }
   ],
   "source": [
    "# total_len = 0\n",
    "# max_len = 0\n",
    "# min_len = 1000000\n",
    "# for line in corpus_contents:\n",
    "#     total_len += len(line)\n",
    "#     if len(line) > max_len:\n",
    "#         max_len = len(line)\n",
    "#     if len(line) < min_len:\n",
    "#         min_len = len(line)\n",
    "# print(max_len / 4)\n",
    "# print(min_len / 4)\n",
    "# print((total_len / len(corpus_contents)) / 4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code is responsible for evaluating our RAG system's retrieval on our two corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = 'gpt-3.5-turbo-1106'\n",
    "llm = 'openai'\n",
    "#how does the number of steps play a role?\n",
    "max_steps = 1\n",
    "#What does the number of documents in the demonstration mean?\n",
    "num_demo = 0\n",
    "#retrieving 8 documents at each step\n",
    "top_k = 8\n",
    "#what is parallel processing? how does the number of threads play a role?\n",
    "thread = 6\n",
    "#load dataset\n",
    "if dataset == 'musique':\n",
    "    data = json.load(open('data/musique.json', 'r'))\n",
    "    corpus = json.load(open('data/musique_corpus.json', 'r'))\n",
    "    prompt_path = 'data/ircot_prompts/musique/gold_with_3_distractors_context_cot_qa_codex.txt'\n",
    "    max_steps = max_steps if max_steps is not None else 4\n",
    "elif dataset == '2wikimultihopqa':\n",
    "    data = json.load(open('data/2wikimultihopqa.json', 'r'))\n",
    "    corpus = json.load(open('data/2wikimultihopqa_corpus.json', 'r'))\n",
    "    prompt_path = 'data/ircot_prompts/2wikimultihopqa/gold_with_3_distractors_context_cot_qa_codex.txt'\n",
    "    max_steps = max_steps if max_steps is not None else 2\n",
    "else:\n",
    "    raise NotImplementedError(f'Dataset {dataset} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = ChatOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"), model=llm_model, temperature=0.0, max_retries=5, timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of demo: 0\n"
     ]
    }
   ],
   "source": [
    "def parse_prompt(file_path: str, has_context=True):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Split the content by the metadata pattern\n",
    "    parts = content.split('# METADATA: ')\n",
    "    parsed_data = []\n",
    "    if has_context:\n",
    "        for part in parts[1:]:  # Skip the first split as it will be empty\n",
    "            metadata_section, rest_of_data = part.split('\\n', 1)\n",
    "            metadata = json.loads(metadata_section)\n",
    "            document_sections = rest_of_data.strip().split('\\n\\nQ: ')\n",
    "            document_text = document_sections[0].strip()\n",
    "            qa_pair = document_sections[1].split('\\nA: ')\n",
    "            question = qa_pair[0].strip()\n",
    "            thought_and_answer = qa_pair[1].strip().split('So the answer is: ')\n",
    "            thought = thought_and_answer[0].strip()\n",
    "            answer = thought_and_answer[1].strip()\n",
    "\n",
    "            parsed_data.append({\n",
    "                'metadata': metadata,\n",
    "                'document': document_text,\n",
    "                'question': question,\n",
    "                'thought_and_answer': qa_pair[1].strip(),\n",
    "                'thought': thought,\n",
    "                'answer': answer\n",
    "            })\n",
    "    else:\n",
    "        for part in parts[1:]:\n",
    "            metadata_section, rest_of_data = part.split('\\n', 1)\n",
    "            metadata = json.loads(metadata_section)\n",
    "            s = rest_of_data.split('\\n')\n",
    "            question = s[0][3:].strip()\n",
    "            thought_and_answer = s[1][3:].strip().split('So the answer is: ')\n",
    "            thought = thought_and_answer[0].strip()\n",
    "            answer = thought_and_answer[1].strip()\n",
    "\n",
    "            parsed_data.append({\n",
    "                'metadata': metadata,\n",
    "                'question': question,\n",
    "                'thought_and_answer': s[1][3:].strip(),\n",
    "                'thought': thought,\n",
    "                'answer': answer\n",
    "            })\n",
    "\n",
    "    return parsed_data\n",
    "few_shot_samples = parse_prompt(prompt_path)\n",
    "few_shot_samples = few_shot_samples[:num_demo]\n",
    "print('num of demo:', len(few_shot_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_1.8.0_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
