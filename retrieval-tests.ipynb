{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amarkanaka/miniconda3/envs/faiss_1.8.0_new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import backoff\n",
    "\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "# from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "from abc import abstractmethod\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    context: str\n",
    "    input: str\n",
    "    output: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(tokenEmbeddings, paddingInfo):\n",
    "    tokenEmbeddingsNoPad = tokenEmbeddings.masked_fill(~paddingInfo[...,None].bool(), 0)\n",
    "    sentenceEmbeddings = tokenEmbeddingsNoPad.sum(dim = 1) / paddingInfo.sum(dim = 1)[...,None]\n",
    "    return sentenceEmbeddings\n",
    "\n",
    "def mean_pooling_embedding_with_normalization(batch_str, tokenizer, model):\n",
    "    encoding = tokenizer(batch_str, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    if(torch.cuda.is_available()):\n",
    "        cuda_device = torch.device(\"cuda\") \n",
    "        input_ids = input_ids.to(cuda_device)\n",
    "        attention_mask = attention_mask.to(cuda_device)\n",
    "    else:\n",
    "        cuda_device = torch.device(\"cpu\") \n",
    "        input_ids = input_ids.to(cuda_device)\n",
    "        attention_mask = attention_mask.to(cuda_device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    sentenceEmbeddings = mean_pooling(outputs[0], attention_mask)\n",
    "    sentenceEmbeddingsNorm = sentenceEmbeddings.divide(torch.linalg.norm(sentenceEmbeddings,dim = 1)[...,None])\n",
    "    return sentenceEmbeddingsNorm    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "# client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "# ircot_reason_instruction = 'You serve as an intelligent assistant, adept at facilitating users through complex, multi-hop reasoning across multiple documents. This task is illustrated through a demonstration consisting of a document set paired with a relevant question and its multi-hop reasoning thoughts, delineated by the string \"Thought\". Your task is to generate one thought for current step, DON\\'T generate all thoughts at once! If you reach what you believe to be the final step, start with \"So the answer is:\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_prompt(file_path: str, has_context=True):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         content = file.read()\n",
    "\n",
    "#     # Split the content by the metadata pattern\n",
    "#     parts = content.split('# METADATA: ')\n",
    "#     parsed_data = []\n",
    "#     if has_context:\n",
    "#         for part in parts[1:]:  # Skip the first split as it will be empty\n",
    "#             metadata_section, rest_of_data = part.split('\\n', 1)\n",
    "#             metadata = json.loads(metadata_section)\n",
    "#             document_sections = rest_of_data.strip().split('\\n\\nQ: ')\n",
    "#             document_text = document_sections[0].strip()\n",
    "#             qa_pair = document_sections[1].split('\\nA: ')\n",
    "#             question = qa_pair[0].strip()\n",
    "#             thought_and_answer = qa_pair[1].strip().split('So the answer is: ')\n",
    "#             thought = thought_and_answer[0].strip()\n",
    "#             answer = thought_and_answer[1].strip()\n",
    "\n",
    "#             parsed_data.append({\n",
    "#                 'metadata': metadata,\n",
    "#                 'document': document_text,\n",
    "#                 'question': question,\n",
    "#                 'thought_and_answer': qa_pair[1].strip(),\n",
    "#                 'thought': thought,\n",
    "#                 'answer': answer\n",
    "#             })\n",
    "#     else:\n",
    "#         for part in parts[1:]:\n",
    "#             metadata_section, rest_of_data = part.split('\\n', 1)\n",
    "#             metadata = json.loads(metadata_section)\n",
    "#             s = rest_of_data.split('\\n')\n",
    "#             question = s[0][3:].strip()\n",
    "#             thought_and_answer = s[1][3:].strip().split('So the answer is: ')\n",
    "#             thought = thought_and_answer[0].strip()\n",
    "#             answer = thought_and_answer[1].strip()\n",
    "\n",
    "#             parsed_data.append({\n",
    "#                 'metadata': metadata,\n",
    "#                 'question': question,\n",
    "#                 'thought_and_answer': s[1][3:].strip(),\n",
    "#                 'thought': thought,\n",
    "#                 'answer': answer\n",
    "#             })\n",
    "\n",
    "#     return parsed_data\n",
    "\n",
    "\n",
    "def num_tokens_by_tiktoken(text: str):\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the retriever in this case?\n",
    "def retrieve_and_generate(idx, sample, dataset, top_k, k_list,max_steps, few_shot_samples, corpus, retriever, client, processed_ids):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State, tokenizer: AutoTokenizer, model: AutoModel):\n",
    "    with torch.no_grad():\n",
    "        query_embedding = mean_pooling_embedding_with_normalization(state[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 768\n",
    "#normalize embeddings before building index using inner product. Note that maximal inner product with normalized embeddings is equivalent to cosine similarity \n",
    "norm = True\n",
    "\n",
    "\"\"\"User-inputted args\"\"\"\n",
    "unit = \"hippo\"\n",
    "dataset = 'musique'\n",
    "max_steps = 1\n",
    "num_demo = 0\n",
    "\n",
    "\n",
    "model_label = 'facebook_contriever'\n",
    "\n",
    "vector_path = f'data/{dataset}/{dataset}_{model_label}_{unit}_vectors_norm.npy'\n",
    "index_path = f'data/{dataset}/{dataset}_{model_label}_{unit}_ip_norm.index'\n",
    "if(os.path.isfile(vector_path)):\n",
    "    vectors = np.load(vector_path)\n",
    "if dataset == 'musique':\n",
    "    corpus = json.load(open('data/musique_corpus.json', 'r'))\n",
    "elif dataset == '2wikimultihopqa':\n",
    "    corpus = json.load(open('data/2wikimultihopqa_corpus.json', 'r'))\n",
    "\n",
    "corpus_contents = List()\n",
    "for item in corpus:\n",
    "    corpus_contents.append(Document(page_content=item[\"text\"], metadata=item[\"title\"]))\n",
    "print('corpus size: {}'.format(len(corpus_contents)))\n",
    "\n",
    "#create sentence-level embeddings using mean-pooling and normalize to prepare for cosine similarity indexing\n",
    "\n",
    "if os.path.isfile(vector_path):\n",
    "    print('Loading existing vectors:', vector_path)\n",
    "    vectors = np.load(vector_path)\n",
    "    print('Vectors loaded:', len(vectors))\n",
    "\n",
    "else:\n",
    "    # load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
    "    model = AutoModel.from_pretrained('facebook/contriever')\n",
    "    # Check if multiple GPUs are available and if so, use them all\n",
    "    \"\"\"CHANGE THIS FOR SERVER RUN\"\"\"\n",
    "    if (torch.cuda.is_available()):\n",
    "        device = torch.device(\"cuda\")    \n",
    "        model.to(device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model.to(device)\n",
    "    #test batch size = 16 and batch size = 32 \n",
    "    batch_size = 16\n",
    "    vectors = np.zeros((len(corpus_contents), dim))\n",
    "    #get batch_size number of entries from corpus_contents, tokenize and embed them in 768 dimensional space\n",
    "    for idx in range(0, len(corpus_contents), batch_size):\n",
    "        end_idx = min(idx + batch_size, len(corpus_contents))\n",
    "        seqs = corpus_contents[idx:end_idx]\n",
    "        try:\n",
    "            #read above comments to understand what this function does\n",
    "            batch_embeddings = mean_pooling_embedding_with_normalization(seqs, tokenizer, model)\n",
    "        except Exception as e:\n",
    "            batch_embeddings = torch.zeros((len(seqs), dim))\n",
    "            print(f'Error at {idx}:', e)\n",
    "        vectors[idx:end_idx] = batch_embeddings.detach().to('cpu').numpy()\n",
    "    print(\"Type of vectors is {}\".format(type(vectors)))\n",
    "    fp = open(vector_path, 'wb')\n",
    "    np.save(fp, vectors)\n",
    "    fp.close()\n",
    "    print('vectors saved to {}'.format(vector_path))\n",
    "\n",
    "    # using FAISS on CPU (GPU support unavailable for mac)\n",
    "    if os.path.isfile(index_path):\n",
    "            print('index file already exists:', index_path)\n",
    "            print('index size: {}'.format(faiss.read_index(index_path).ntotal))\n",
    "    else:\n",
    "        print('Building index...')\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "        vectors = vectors.astype('float32')\n",
    "        index.add(vectors)\n",
    "\n",
    "        # save faiss index to file\n",
    "        fp = open(index_path, 'w')\n",
    "        faiss.write_index(index, index_path)\n",
    "        print('index saved to {}'.format(index_path))\n",
    "        print('index size: {}'.format(index.ntotal))\n",
    "\n",
    "llm_model = 'gpt-3.5-turbo-1106'\n",
    "llm = 'openai'\n",
    "\"\"\"\n",
    "User-inputted args\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "For 2wikimultihopqa, change max_steps to 1, num_demo to 1 to perform multistep retrieval.\n",
    "For musique, change max_steps to 3, num_demo 1 to perform multistep retrieval.\n",
    "\"\"\"\n",
    "max_steps = 1\n",
    "num_demo = 1\n",
    "top_k = 8\n",
    "#load dataset\n",
    "if dataset == 'musique':\n",
    "    data = json.load(open('data/musique.json', 'r'))\n",
    "    corpus = json.load(open('data/musique_corpus.json', 'r'))\n",
    "    prompt_path = 'data/ircot_prompts/musique/gold_with_3_distractors_context_cot_qa_codex.txt'\n",
    "    max_steps = max_steps if max_steps is not None else 4\n",
    "elif dataset == '2wikimultihopqa':\n",
    "    data = json.load(open('data/2wikimultihopqa.json', 'r'))\n",
    "    corpus = json.load(open('data/2wikimultihopqa_corpus.json', 'r'))\n",
    "    prompt_path = 'data/ircot_prompts/2wikimultihopqa/gold_with_3_distractors_context_cot_qa_codex.txt'\n",
    "    max_steps = max_steps if max_steps is not None else 2\n",
    "else:\n",
    "    raise NotImplementedError(f'Dataset {dataset} not implemented')\n",
    "\n",
    "# doc_ensemble = ''\n",
    "# doc_ensemble_str = 'doc_ensemble' if doc_ensemble else 'no_ensemble'\n",
    "doc_ensemble_str = ''\n",
    "alt_model_label = 'facebook_contriever'\n",
    "if max_steps > 1:\n",
    "    k_list = [1, 2, 5, 8]\n",
    "    output_path = f'output/ircot/{dataset}_{alt_model_label}_demo_{num_demo}_{llm_model}_{doc_ensemble_str}_step_{max_steps}_top_{top_k}.json'\n",
    "else:  # only one step\n",
    "    top_k = 100\n",
    "    output_path = f'output/proposition_{dataset}_{alt_model_label}_{doc_ensemble_str}.json'\n",
    "    k_list = [1, 2, 5, 10, 15, 20, 30, 50, 100]\n",
    "\n",
    "if dataset == 'musique':\n",
    "    faiss_index = faiss.read_index('data/musique/musique_facebook_contriever_proposition_ip_norm.index')\n",
    "else:\n",
    "    faiss_index = faiss.read_index('data/2wikimultihopqa/2wikimultihopqa_facebook_contriever_proposition_ip_norm.index')\n",
    "    \n",
    "model_label = 'facebook/contriever'\n",
    "\n",
    "\n",
    "total_recall = {k: 0 for k in k_list}\n",
    "\n",
    "results = data\n",
    "processed_ids = set()\n",
    "\n",
    "load_dotenv('.env')\n",
    "# print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "#Create OpenAI Client\n",
    "# client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "#examples for model\n",
    "# few_shot_samples = parse_prompt(prompt_path)\n",
    "# few_shot_samples = few_shot_samples[:num_demo]\n",
    "# print('num of demo:', len(few_shot_samples))\n",
    "\n",
    "k_list = [1, 2, 5, 8]\n",
    "\n",
    "total_recall = {k: 0 for k in k_list}\n",
    "processed_ids = set()\n",
    "\n",
    "#requests per minute: 500\n",
    "#tokens per minute: 10,000 \n",
    "\n",
    "#few-shot prompting\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature = 0.0)\n",
    "# examples = [{\"input\":f'{few_shot_samples[\"document\"]}\\n\\nQuestion: {few_shot_samples[\"question\"]}'}, {\"output\":f'Answer: {few_shot_samples[\"thought_and_answer\"]}'}]\n",
    "\n",
    "\n",
    "# Application Logic - multistep retrieval\n",
    "# example_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"human\", \"{input}\"),\n",
    "#         (\"ai\", \"{output}\"),\n",
    "#     ]\n",
    "# )\n",
    "# few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "#     example_prompt=example_prompt,\n",
    "#     examples=examples,\n",
    "# )\n",
    "\n",
    "# final_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"You are an assistant for question-answering tasks, adept at facilitating users through complex, multi-hop reasoning across multiple pieces of information. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"),\n",
    "#         few_shot_prompt,\n",
    "#         (\"human\", \"{input}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "#application logic - single step retrieval\n",
    "\n",
    "\n",
    "\"\"\"how to use:\n",
    "chain = final_prompt | model\n",
    "chain.invoke({\"input\": \"What is 2 🦜 9?\"})\n",
    "\"\"\"\n",
    "\n",
    "#tokenizer and embedding model\n",
    "if(torch.cuda.is_available()):\n",
    "    device  = torch.device(\"cuda\") \n",
    "else:\n",
    "    device  = torch.device(\"cpu\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_label).to(device)\n",
    "model = AutoModel.from_pretrained(model_label).to(device)\n",
    "\n",
    "\n",
    "for idx in range(len(data)):\n",
    "    #change this logic\n",
    "    idx, recall, retrieved_passages, thoughts, it = retrieve_and_generate(idx, data[idx], dataset, top_k, k_list,max_steps, few_shot_samples, corpus, retriever, client, processed_ids) \n",
    "    # print metrics\n",
    "    for k in k_list:\n",
    "        total_recall[k] += recall[k]\n",
    "        print(f'R@{k}: {total_recall[k] / (idx + 1):.4f} ', end='')\n",
    "    print()\n",
    "    if max_steps > 1:\n",
    "        print('[ITERATION]', it, '[PASSAGE]', len(retrieved_passages), '[THOUGHT]', thoughts)\n",
    "    \n",
    "    # record results\n",
    "    results[idx]['retrieved'] = retrieved_passages\n",
    "    results[idx]['recall'] = recall\n",
    "    results[idx]['thoughts'] = thoughts\n",
    "        \n",
    "    # if idx % 50 == 0:\n",
    "    #     f = open(output_path, 'w')\n",
    "    #     json.dump(results, f)\n",
    "    #     f.close()\n",
    "\n",
    "# save results\n",
    "f = open(output_path, 'w')\n",
    "json.dump(results, f)\n",
    "f.close()\n",
    "print(f'Saved results to {output_path}')\n",
    "for k in k_list:\n",
    "    #average recall (across 1,000 questions for musique)\n",
    "    print(f'R@{k}: {total_recall[k] / len(data):.4f} ', end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_1.8.0_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
